{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMawXRVmGT7U"
   },
   "source": [
    "# 1 - Problem statement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXFnc8_sbTwj"
   },
   "source": [
    "It is January 2022.\n",
    "\n",
    "You just started your new position: you've joined the Applied Machine Learning team of an Italian utility. \n",
    "\n",
    "You barely had the time to meet your new colleagues and you already have your first task. Up until now, the company relied on an external service to forecast the power demand of its customers. It is now time to internalize. \n",
    "\n",
    "Power demand forecasting is a critical task for every utility: power storage is not cheap nor widely available, so the balance of the grid must be guaranteed at any time. The production must match the demand. In Italy, this is ensured by the free power market, where utilities can trade power production, and by *ad-hoc* actions performed by the Transmission System Operator (TSO).\n",
    "\n",
    "To safeguard the smooth operation of the system, utilities must produce day-ahead hourly forecasts of the power demand of their customers, and they get financial penalties for errors. \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Simplification.</b> \n",
    "\n",
    "We will consider **daily forecasting of the whole Italian load**, instead of hourly forecast of the demand from the customers of a specific company.\n",
    "Moreover, instead of day-ahead we will focus on the **long term forecasting, one year in advance**.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4Yj4WlSGT7W"
   },
   "source": [
    "# 2 - Data retrieval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuQVSHGrbGY8"
   },
   "source": [
    "Historical power load data can be retrieved from the [ENTSO-E portal](https://www.entsoe.eu/data/power-stats/), while newer series are avaiable on the websites of the national TSOs. In Italy, the TSO is Terna, and it [publishes power load and its own forecast](https://www.terna.it/en/electric-system/transparency-report/total-load).\n",
    "\n",
    "After a bit of data wrangling you manage to get all historical data into a single csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ywyIOjj8QDjs",
    "outputId": "8a38febd-98ff-4a14-cbf0-bd3300bb42be"
   },
   "outputs": [],
   "source": [
    "import gdown\n",
    "\n",
    "url = \"https://drive.google.com/drive/folders/1esN41Xw_XfKgNh6cxj9YVXJCAtQhaAP_?usp=sharing\"\n",
    "gdown.download_folder(url, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rCtF2lwBGT7Y"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "c3KzGXkqGT7Y",
    "outputId": "9a3e1f01-1232-4f58-e882-790a8e853220"
   },
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv(\"data/2006_2021_data.csv\", index_col=0, parse_dates=True)\n",
    "raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXfrbVQ_GT7Y"
   },
   "source": [
    "Since you care about daily data, you sum the load on the same day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6iazRTA2KkeL"
   },
   "outputs": [],
   "source": [
    "resampled_df = raw_df.resample('D').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RX_JHsWxGT7Z",
    "outputId": "e39a053b-b263-4c0b-980b-9379855bf3ba"
   },
   "outputs": [],
   "source": [
    "load_series = resampled_df.Load\n",
    "load_series.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Km6fmILuGT7Z"
   },
   "source": [
    "# 3 - Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_wNlzC0bJM0"
   },
   "source": [
    "You did your homework: the features of the power load series are subject of an extensive literature, so you already know what to look for.\n",
    "\n",
    "You start by plotting the series with an interactive view for a quick visual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "1qJ8hPFSGT7Z",
    "outputId": "cd9bab2d-b913-45a9-ef6a-8b8401a23172"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "px.line(load_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5TyNvgjPGT7a"
   },
   "source": [
    "Some features are immediately visible:\n",
    "- the trend is decreasing\n",
    "- there are weekly and yearly seasonalities\n",
    "- summer is the period of highest consumption, due to air conditioning\n",
    "- there are some drops in correspondence with holidays, which reduce the demand from industrial plants\n",
    "- the size of the peak changes from year to year, possibly due to weather conditions\n",
    "- there is a pretty big drop in demand around march 2020\n",
    "\n",
    "You further explore the trend with a moving average filter, in order to remove high-frequency oscillations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "zdSZmN8bGT7a",
    "outputId": "590a5e1a-80a4-499d-d0c2-7224fcf7a30a"
   },
   "outputs": [],
   "source": [
    "px.line(load_series.rolling(365).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOeTz1GQGT7a"
   },
   "source": [
    "You then have a look at the **autocorrelation** function, a common statistical analysis which is very useful to detect and quantify seasonality patterns.\n",
    "\n",
    "The autocorrelation of a real-valued stochastic process $X(t)$ is a function of two time variables, defined as \n",
    "$$\n",
    "R_{XX}(t_1, t_2) = \\mathbb{E}\\left[X_{t_1}X_{t_2} \\right],\n",
    "$$\n",
    "and it measures the Pearson correlation between values of the $X$ process taken at those two times.\n",
    "\n",
    "In practice, usually we estimate the autocorrelation of a single realization of a process, i.e. a time series of the form $\\left\\{x(t)\\right\\}_{t=1}^n$, as a function of the difference $k = t_1 - t_2$, often called **lag**:\n",
    "$$\n",
    "\\hat{R}(k)=\\frac{1}{(n-k) \\sigma^2} \\sum_{t=1}^{n-k}\\left(x(t)-\\mu\\right)\\left(x(t+k)-\\mu\\right)\n",
    "$$\n",
    "where $\\mu$ and $\\sigma$ are also estimated from the time series.\n",
    "\n",
    "A high autocorrelation value for a given lag $k$ means that the series is highly correlated with a version of itself that has been shifted $k$ steps, which can be intuitively interpreted as the presence of a strong repetitive component having frequency $1/k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "gq9UG6rILu9i",
    "outputId": "c7999e4e-c123-47ca-b7b2-249ceeaf44c6"
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "plot_acf(load_series, lags=370);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zAwsmdLUL0RM"
   },
   "source": [
    "Let's focus on the first lags to better appreciate the highest autocorrelation values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "yAN0JDWeGT7a",
    "outputId": "bbf7f9a3-8ba9-4513-b1f9-55a9f06b9e0a"
   },
   "outputs": [],
   "source": [
    "plot_acf(load_series, lags=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zAH1DZipGT7b"
   },
   "source": [
    "The ACF shows a weekly seasonality, as well as a longer periodicity, which we may assume to be yearly. This fact can be confirmed by plotting the periodogram, an estimator of the spectral power density of the time series. The analysis is here omitted.\n",
    "\n",
    "Finally, you plot the power demand juxtaposed year-over-year, a view that gives interesting clues on trend and holiday effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "uXsKHHXyGT7b",
    "outputId": "ec8b5017-5ff1-46f7-bcd7-66bd809c7743"
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import datetime\n",
    "\n",
    "year_over_year_df = pd.DataFrame({\n",
    "    'load': load_series,\n",
    "    'day_in_year': load_series.index.dayofyear,\n",
    "    'day_in_week': load_series.index.dayofweek, # Monday = 0, Sunday = 6\n",
    "    'year': load_series.index.year\n",
    "})\n",
    "fig = go.Figure(\n",
    "    layout=dict(\n",
    "        title='Adjusted day in year - if each year started on Monday'\n",
    "    )\n",
    ")\n",
    "palette = iter(px.colors.sequential.Viridis[2:])\n",
    "for year, year_df in year_over_year_df.groupby('year'):\n",
    "  if year >= 2015:\n",
    "    year_df.day_in_year += year_df.day_in_week.iloc[0]\n",
    "    fig.add_trace(go.Scatter(x=year_df.day_in_year, y=year_df.load, name=year, marker_color=next(palette)))\n",
    "\n",
    "customdata = pd.date_range(datetime.date(2021, 1, 1), datetime.date(2021, 12, 31))\n",
    "hovertemplate = ('Date: %{customdata|%d %B}<br>' + \n",
    "    'Value: %{y}' + \n",
    "    '<extra></extra>')\n",
    "\n",
    "fig.update_traces(customdata=customdata, hovertemplate=hovertemplate)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ZvIFScyS7vT"
   },
   "source": [
    "# 4 - Modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alHWdctbbNbR"
   },
   "source": [
    "\n",
    "This is the main step: it involves choosing a model, i.e. a parameterized family of functions which you hope approximates well the target series, and then running some (training) algorithm that finds the best candidate within the family.\n",
    "\n",
    "Normally you should try different models and then compare them to pick the best; here you will use only one.\n",
    "\n",
    "In practice, the above translates into the following steps:\n",
    "- splitting the dataset at a certain date, into two chunks;\n",
    "- using the first chunk to train the model, i.e. determining the parameters that provide the best fit to it;\n",
    "- using the second chunk to test how good the fit is when applied to unseen data.\n",
    "\n",
    "This subdivision is fundamental if you want to make sure that the model can generalize, i.e. perform well on new data that comes from the same generating process but exhibits different behaviors due to the stochastic nature of the process itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qco2y0hoobLc"
   },
   "source": [
    "## The model: Fourier regression\n",
    "\n",
    "Many methods for long-term forecasting aim at identifying and reconstructing the trend and seasonal structure of the series.\n",
    "When seasonal components are important, as in this case, a common approach is [Fourier regression](https://otexts.com/fpp3/useful-predictors.html#fourier-series).\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Simplification.</b> \n",
    "\n",
    "There are many academic papers and book chapters about the application of Fourier regression for time series forecasting: the interested reader may refer to [A. Incremona, Machine Learning methods for long and short term energy demand forecasting](https://iris.unipv.it/retrieve/handle/11571/1436355/470615/Incremona_PhD_Thesis.pdf), [A. Incremona, G. De Nicolao, Spectral Characterization of the Multi-Seasonal Component of the Italian Electric Load: A LASSO-FFT Approach](https://ieeexplore.ieee.org/abstract/document/8734852), and [A. Guerini, G. De Nicolao, Long-term electric load forecasting: A torus-based approach](https://ieeexplore.ieee.org/abstract/document/7330957).\n",
    "</div>\n",
    "\n",
    "Here, you opt for a simple unregularized linear model:\n",
    "$$\n",
    "\\hat{x}(t) = \\mu + \\kappa t + \\sum_{i=1}^n\\left[\\alpha_i\\sin(2\\pi\\frac{i}{7}t) + \\beta_i\\cos(2\\pi\\frac{i}{7}t)\\right] + \\sum_{j=1}^m\\left[\\gamma_j\\sin(2\\pi\\frac{j}{365.25}t) + \\delta_j\\cos(2\\pi\\frac{j}{365.25}t)\\right] + \\tau \\chi_t\n",
    "$$\n",
    "where:\n",
    "- the terms $\\mu + \\kappa t$ model a linear trend\n",
    "- $\\chi_t$ is the indicator function taking value 1 when date $t$ is a holiday and 0 otherwise\n",
    "- the sums\n",
    "$$\n",
    "\\sum_{i=1}^n\\left[\\alpha_i\\sin(2\\pi\\frac{i}{7}t) + \\beta_i\\cos(2\\pi\\frac{i}{7}t)\\right]\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\sum_{j=1}^m\\left[\\gamma_i\\sin(2\\pi\\frac{j}{365.25}t) + \\delta_j\\sin(2\\pi\\frac{j}{365.25}t)\\right]\n",
    "$$\n",
    "model the weekly and yearly periodicity\n",
    "\n",
    "The parameters of the model are $\\mu$, $\\kappa$, $\\{\\alpha_i\\}$, $\\{\\beta_i\\}$, $\\{\\gamma_j\\}$, $\\{\\delta_j\\}, \\tau$, which can all be trained by least squares. \n",
    "\n",
    "The hyperparameters are $n$ and $m$, which can be chosen by cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BMqMEa0STBco"
   },
   "outputs": [],
   "source": [
    "import holidays\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_VaWb11ot0C"
   },
   "source": [
    "## Feature preparation\n",
    "To perform a Fourier regression, you need proper inputs. Therefore, you build a new dataset.\n",
    "\n",
    "You add the trend and the holiday dummy, then you build the Fourier harmonics.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Simplification.</b> \n",
    "\n",
    "In the real world, it would be good practice to choose the number of harmonics with data-driven methods: cross-validation is one, LASSO is another.\n",
    "Here we just set a fixed value, which we know performs good enough for our purposes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzSP2190otTH"
   },
   "outputs": [],
   "source": [
    "n_harmonics = {\n",
    "    '7': 5,\n",
    "    '365': 20\n",
    "}\n",
    "\n",
    "dates = load_series.index\n",
    "regression_df = pd.DataFrame(index=dates)\n",
    "\n",
    "# create holiday series, valued 0 or 1 depending on the day\n",
    "holiday_cal = holidays.Italy()\n",
    "regression_df['holiday'] = dates.to_series().apply(lambda s: s in holiday_cal).astype(int)\n",
    "\n",
    "# turn dates into integer incremental values to be used as the time axis\n",
    "date_origin = pd.to_datetime('2006-01-01')\n",
    "regression_df['trend'] = (dates - date_origin).days\n",
    "\n",
    "# create fourier terms as trigonometric functions of various frequencies\n",
    "for i in range(1, n_harmonics['7'] + 1):\n",
    "    regression_df[f'sin_7_{i}'] = np.sin(2 * np.pi * i / 7 * regression_df['trend'])\n",
    "    regression_df[f'cos_7_{i}'] = np.cos(2 * np.pi * i / 7 * regression_df['trend'])\n",
    "for i in range(1, n_harmonics['365'] + 1):\n",
    "    regression_df[f'sin_365_{i}'] = np.sin(2 * np.pi * i / 365.25 * regression_df['trend'])\n",
    "    regression_df[f'cos_365_{i}'] = np.cos(2 * np.pi * i / 365.25 * regression_df['trend'])\n",
    "\n",
    "# join all features together\n",
    "regression_data_df = resampled_df.join(regression_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6S94KqTro-EK"
   },
   "source": [
    "## Model training\n",
    "\n",
    "Training the model is often done by using well-known libraries, such as [scikit-learn](https://scikit-learn.org/stable/); this makes everything straightforward, although it arguably hides the inner structure and complexity that ML algorithms have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6JxoWGMQona9"
   },
   "outputs": [],
   "source": [
    "TRAIN_END = '2020-12-31 23:59'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "id": "-Hf_nY-b-P26",
    "outputId": "3339c99b-b56a-405a-88fc-d031a5edff36"
   },
   "outputs": [],
   "source": [
    "# split data into train and test\n",
    "train_df, test_df = regression_data_df[:TRAIN_END], regression_data_df[TRAIN_END:]\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KMGmlsBAqjik"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ny23x3MwoqKb"
   },
   "outputs": [],
   "source": [
    "# split training set into features and target\n",
    "train_x, train_y = train_df.drop(columns='Load'), train_df['Load']\n",
    "test_x, test_y = test_df.drop(columns='Load'), test_df['Load']\n",
    "\n",
    "# instantiate and train scikit-learn's LinearRegression model\n",
    "model = LinearRegression()\n",
    "fit = model.fit(train_x, train_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCxtLUqaTpJ5"
   },
   "source": [
    "After training you save the model in **pickle** format, a convenient way of storing python serialized objects that allows to load them at a later time in the same exact state, already instantiated and ready to be used again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DaZxDronPrdu"
   },
   "outputs": [],
   "source": [
    "# save the model into a serialized format, ready to be used when needed\n",
    "import pickle\n",
    "with open(\"trained_model.pkl\", 'wb') as f:\n",
    "    pickle.dump(fit, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNH0M8rM6Gq4"
   },
   "source": [
    "## Model inspection\n",
    "\n",
    "Let's take a look at the output of the model, that is the parameters that were selected by the training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wz0rJvnG6JLL"
   },
   "outputs": [],
   "source": [
    "# extract information on estimated coefficients into a dataframe\n",
    "model_output_df = pd.DataFrame(index=pd.Index([\"intercept\"] + list(fit.feature_names_in_), name=\"component\"), data={\"coefficient\": [fit.intercept_] + list(fit.coef_)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "1ne64fR4cnJL",
    "outputId": "52c153f4-6f5d-4410-b0b3-6d09cda44aac"
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact, widgets\n",
    "\n",
    "fig = px.bar(\n",
    "    np.abs(model_output_df),\n",
    "    title = \"Absolute value of estimated coefficients\",\n",
    "    log_y=True\n",
    "       )\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QoMQ6sCVVK3W"
   },
   "source": [
    "Notice the very high value for some coefficients: this is because basically all features but the trend are bounded in the $[-1, 1]$ interval, and the time series has very high values itself.\n",
    "\n",
    "It is usually a good idea therefore to standardize all the series, i.e. both target and features, so that they lie within the same range when fed to a ML model. This reduces numerical problems and optimization issues, and it is even required for some ML model that intrinsically take into account the scale of values.\n",
    "\n",
    "Of course the standardization must be an invertible transformation (it is usually linear) so that one can recover the actual values afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fx2A8FjHWh1G"
   },
   "source": [
    "You can also sum the components pertaining to the same seasonality in order to view the \"profile\" that the model think represents best that seasonal pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 574,
     "referenced_widgets": [
      "415b43c96776411ca7f9f95286b4e1a3",
      "cc370d6b8d4044fbb05ff0585b37a464",
      "d2321f61805440ecab36b5977d448be8",
      "d79e78b85467451ea0b945e90f0d66e0",
      "16b77e8beefb4fefb06513105c085901",
      "e163bda90dc448c0994a37a054304944",
      "fad94c7735724089befe77899472195c"
     ]
    },
    "id": "YT6ue51xhEyb",
    "outputId": "6bc9f835-138b-4602-e505-0de94a2b06d5"
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "selector_widget = widgets.Dropdown(\n",
    "        description=f'Frequency:   ',\n",
    "        value=7,\n",
    "        options=[7, 365]\n",
    "    )\n",
    "@interact(frequency=selector_widget, name=\"Select frequency\")\n",
    "def plot_component(frequency):\n",
    "  if frequency==7:  # align index so that it starts on monday\n",
    "    start = 7 - date_origin.weekday()\n",
    "  else:\n",
    "    start = 0\n",
    "  index = np.arange(start, start + frequency)\n",
    "  component = pd.Series(0, index=index)\n",
    "\n",
    "  # reconstruct the predicted sum of components having the selected frequency\n",
    "  for i in range(1, n_harmonics[str(frequency)] + 1):\n",
    "\n",
    "    # sin component\n",
    "    sin_coeff = model_output_df.at[f\"sin_{frequency}_{i}\", \"coefficient\"]\n",
    "    harmonic_sin = np.sin(2 * np.pi * i / frequency * index) * sin_coeff\n",
    "    component += harmonic_sin\n",
    "\n",
    "    # cos component\n",
    "    cos_coeff = model_output_df.at[f\"cos_{frequency}_{i}\", \"coefficient\"]\n",
    "    harmonic_cos = np.cos(2 * np.pi * i / frequency * index) * cos_coeff\n",
    "    component += harmonic_cos\n",
    "\n",
    "  # set date index for better chart readability\n",
    "  date_index = pd.date_range(\n",
    "      date_origin + pd.offsets.DateOffset(days=start),\n",
    "      date_origin + pd.offsets.DateOffset(days=start + frequency - 1)\n",
    "      )\n",
    "  component.index = date_index\n",
    "  fig = px.line(component,\n",
    "                title = f\"Reconstruction of seasonality {frequency}\",\n",
    "                width=1100)\n",
    "  if frequency==7:\n",
    "    fig.update_layout(xaxis = dict(tickformat = '%A'))\n",
    "  elif frequency==365:\n",
    "    fig.update_layout(xaxis = dict(tickformat = '%e %B'))\n",
    "  fig.update_layout(showlegend=False)\n",
    "  fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNXAIVAoWvTx"
   },
   "source": [
    "You immediately recognize some observations you made during the explorative analysis, such as the drop in the weekends and close to the holidays. \n",
    "\n",
    "Seems like the model picked them up as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPJJVUgzp57u"
   },
   "source": [
    "## Prediction and evaluation\n",
    "Finally, you use the trained model to perform some predictions and evaluate the results, in order to have a quantitative measure for comparing different models and ultimately validating the best one. \n",
    "\n",
    "There are many metric functions you can use, and the choice really depends on the use case; for now, you settle on the Mean Absolute Percentage Error:\n",
    "$$\n",
    "\\mathrm{MAPE(x, \\hat{x})}=\\frac{100 \\%}{n} \\sum_{t=1}^n\\left|\\frac{x(t)-\\hat{x}(t)}{x(t)}\\right|\n",
    "$$\n",
    "where $x$ and $\\hat{x}$ are the actual and forecasted series respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "TEyHStxWquH0",
    "outputId": "51b8c325-c69a-4223-fc41-c182d3a062fd"
   },
   "outputs": [],
   "source": [
    "# apply model on test dates\n",
    "test_predictions = model.predict(test_x)\n",
    "test_results_df = pd.DataFrame({\"actual\": test_df.Load, \"predicted\": test_predictions})\n",
    "px.line(test_results_df, title=\"Predictions on the test set\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMNUV7lcYI3F"
   },
   "source": [
    "From a visual inspection on the test set, the prediction seem to be good. There's some major mistakes in some holiday dates, especially on the 15th of august 2021.\n",
    "\n",
    "Upon some further inspection you realize that that day was both a holiday and a sunday: your additive model accounted for both these conditions with two negative components, but in real life they are basically idempotent (a factory is closed on sunday and on a holiday, but the effect on its electricity consumption is not doubled!)\n",
    "\n",
    "This maybe sheds some doubts on using a linear model, wheres the target series seems to be sometimes not linear on its features.\n",
    "\n",
    "You take note of this, and proceed with the MAPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uAOl3CqKroD0",
    "outputId": "dff3abe2-158a-4159-b5e9-16290bd86d12"
   },
   "outputs": [],
   "source": [
    "# compute MAPE on test predictions\n",
    "np.mean(np.abs(test_results_df[\"actual\"] - test_results_df[\"predicted\"]) / test_results_df[\"predicted\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNFyIOH5X4Al"
   },
   "source": [
    "Just under 4% is not a bad result at all, considering you skipped many best practices and only tried a single model with one set of hyperparameters. \n",
    "\n",
    "Good enough!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgUO_qMTvKF1"
   },
   "source": [
    "## Goodness of fit\n",
    "In order to assess the validity of a model, it is useful to plot actual vs predicted values and see how much they resemble an identity, which is plotted as the bisector of the first quadrant in the following chart.\n",
    "\n",
    "To help assess this, a line is fit to the scatter data by minimizing the sum of squared distances (OLS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "id": "r8QWFvCMWnr_",
    "outputId": "4437d314-8581-4989-bebc-275b2379fa26"
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(test_results_df, x=\"actual\", y=\"predicted\", trendline=\"ols\", title=\"Goodness of fit\", height=800, width=900)\n",
    "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "fig.data[-1].marker.color = \"black\"\n",
    "fig.data[-1].name = \"OLS fit\"\n",
    "fig.data[-1].showlegend = True\n",
    "fig.add_trace(go.Scatter(x= np.arange(500000, 1000000, 5000), y=np.arange(500000, 1000000, 5000), mode = 'lines', name=\"actual=predicted\"))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VoLcGXbLwVJj"
   },
   "source": [
    "Similar information can be delivered by plotting the prediction errors (or residuals) against time in order to check \"problematic\" dates or periods where the error lies too far from the ideal zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3fiQrVDdw-ig"
   },
   "outputs": [],
   "source": [
    "resid_series = test_results_df[\"actual\"] - test_results_df[\"predicted\"]\n",
    "resid_series.name = \"Residual\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "FVPAFJdrw6SZ",
    "outputId": "5554984b-31d4-4d85-bc05-d9d7738c16a9"
   },
   "outputs": [],
   "source": [
    "px.line(resid_series, title=\"Difference between actual and predicted values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "goYZJhDCwx8s"
   },
   "source": [
    "Lastly, you apply the same autocorrelation plot you used during EDA on the residual series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "fFT_EaFfaFLP",
    "outputId": "783fc15d-b4a6-468d-f467-9ed62e8cd114"
   },
   "outputs": [],
   "source": [
    "plot_acf(\n",
    "    resid_series,\n",
    "    lags=50,\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BW9w8DkoZ2oK"
   },
   "source": [
    "It confirms that the residual carries much less information than the original in terms of repeating patterns; this means you extracted much of the predictability that was encoded in the data, although you could definitely do something better as low lags still show significant autocorrelation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOipc_LSvPDq"
   },
   "source": [
    "# 5 - Model monitoring\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVhvT6UtbQcA"
   },
   "source": [
    "After settling for a satisfactory model, you need to keep monitoring its performance while it's being used by the company.\n",
    "\n",
    "For this, you set up a rolling measurement (moving average) of the model's performance, which tells you if it is behaving as expected or you need to worry about it, and potentially try better approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7VIMlf253G61"
   },
   "outputs": [],
   "source": [
    "future_df = pd.read_csv(\"data/2022_data.csv\", index_col=0, parse_dates=True).resample('D').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B0IFZBLl3pyU"
   },
   "outputs": [],
   "source": [
    "future_dates = future_df.index\n",
    "future_regression_df = pd.DataFrame(index=future_dates)\n",
    "\n",
    "future_regression_df['holiday'] = future_dates.to_series().apply(lambda s: s in holiday_cal).astype(int)\n",
    "\n",
    "future_regression_df['trend'] = (future_dates - date_origin).days\n",
    "\n",
    "for i in range(1, n_harmonics['7'] + 1):\n",
    "    future_regression_df[f'sin_7_{i}'] = np.sin(2 * np.pi * i / 7 * future_regression_df['trend'])\n",
    "    future_regression_df[f'cos_7_{i}'] = np.cos(2 * np.pi * i / 7 * future_regression_df['trend'])\n",
    "for i in range(1, n_harmonics['365'] + 1):\n",
    "    future_regression_df[f'sin_365_{i}'] = np.sin(2 * np.pi * i / 365.25 * future_regression_df['trend'])\n",
    "    future_regression_df[f'cos_365_{i}'] = np.cos(2 * np.pi * i / 365.25 * future_regression_df['trend'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i_FUOuyU3Kr_"
   },
   "outputs": [],
   "source": [
    "future_predictions = fit.predict(future_regression_df)\n",
    "future_results_df = pd.DataFrame({\"actual\": future_df.Load, \"predicted\": future_predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o7Io1qDyvRBr",
    "outputId": "adebf28c-5404-4133-d3e0-2feacc1948fe"
   },
   "outputs": [],
   "source": [
    "window_size = 3\n",
    "extended_future_results_df = pd.concat([test_results_df[-window_size:], future_results_df], axis=0)\n",
    "rolling_mape_df = pd.DataFrame(\n",
    "    {'rolling_mape': map(lambda w: np.mean(np.abs(w[\"actual\"] - w[\"predicted\"]) / w[\"predicted\"]),\n",
    "                         extended_future_results_df.rolling(window_size))},\n",
    "    index=extended_future_results_df.index\n",
    ")\n",
    "fig = px.line(rolling_mape_df[window_size:], title=f\"{window_size} days rolling MAPE in 2022\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "il6VCOTtaKsv"
   },
   "source": [
    "Looks like everything is under control, at least for the first month.\n",
    "\n",
    "It's time to take the next steps, and reorganize all this code making it better, more stable and maintainable.\n",
    "\n",
    "You wish you could only work in notebooks like this, since it makes everything quick and straightforward, but being a good data scientist sometimes means being a good programmer as well :'("
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
