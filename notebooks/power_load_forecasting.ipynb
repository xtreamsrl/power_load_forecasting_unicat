{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMawXRVmGT7U"
   },
   "source": [
    "# 1 - Problem statement\n",
    "It is January 2022.\n",
    "\n",
    "You just started your new position: you've joined the Applied Machine Learning team of an Italian utility. \n",
    "\n",
    "You barely had the time to meet your new colleagues and you already have your first task. Up until now, the company relied on an external service to forecast the power demand of its customers. It is now time to internalize. \n",
    "\n",
    "Power demand forecasting is a critical task for every utility: power storage is not cheap nor widely available, so the balance of the grid must be guaranteed at any time. The production must match the demand. In Italy, this is ensured by the free power market, where utilities can trade power production, and by *ad-hoc* actions performed by the Transmission System Operator (TSO).\n",
    "\n",
    "To safeguard the smooth operation of the system, utilities must produce day-ahead hourly forecasts of the power demand of their customers, and they get financial penalties for errors. \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Simplification.</b> \n",
    "\n",
    "We will consider daily forecasting of the whole Italian load, instead of hourly forecast of the demand from the customers of a specific company.\n",
    "Moreover, instead of day-ahead forecasting we will focus on the long term.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4Yj4WlSGT7W"
   },
   "source": [
    "# 2 - Data retrieval\n",
    "Historical power load data can be retrieved from the [ENTSO-E portal](https://www.entsoe.eu/data/power-stats/), while newer series are avaiable on the websites of the national TSOs. In Italy, the TSO is Terna, and it [publishes power load and its own forecast](https://www.terna.it/en/electric-system/transparency-report/total-load).\n",
    "\n",
    "After a bit of data wrangling you manage to get all historical data into a single csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ywyIOjj8QDjs",
    "outputId": "5ebd3244-a0f7-408f-c85c-8596c7c65c8a"
   },
   "outputs": [],
   "source": [
    "import gdown\n",
    "\n",
    "url = \"https://drive.google.com/drive/folders/1esN41Xw_XfKgNh6cxj9YVXJCAtQhaAP_?usp=sharing\"\n",
    "gdown.download_folder(url, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rCtF2lwBGT7Y"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "c3KzGXkqGT7Y",
    "outputId": "3ac32505-1011-4a52-8e1c-51f88d37d68a"
   },
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv(\"data/2006_2021_data.csv\", index_col=0, parse_dates=True)\n",
    "raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXfrbVQ_GT7Y"
   },
   "source": [
    "Since you care about daily data, you sum the load on the same day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6iazRTA2KkeL"
   },
   "outputs": [],
   "source": [
    "resampled_df = raw_df.resample('D').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RX_JHsWxGT7Z",
    "outputId": "61b4b969-9240-4c18-e6f5-262a7b6ebd20"
   },
   "outputs": [],
   "source": [
    "load_series = resampled_df.Load\n",
    "load_series.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Km6fmILuGT7Z"
   },
   "source": [
    "# 3 - Exploratory Data Analysis\n",
    "You did your homework: the features of the power load series are subject of an extensive literature, so you already know what to look for.\n",
    "\n",
    "You start by plotting the series with an interactive view for a quick visual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "1qJ8hPFSGT7Z",
    "outputId": "5ea7a7ea-671f-4b87-c55d-0e28283da3e9"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "px.line(load_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5TyNvgjPGT7a"
   },
   "source": [
    "Some features are immediately visible:\n",
    "- the trend is decreasing\n",
    "- there are weekly and yearly seasonalities\n",
    "- summer is the period of highest consumption, due to air conditioning\n",
    "- there are some drops in correspondence with holidays, which reduce the demand from industrial plants\n",
    "- the size of the peak changes from year to year, possibly due to weather conditions\n",
    "- there is a pretty big drop in demand around march 2020\n",
    "\n",
    "You further explore the trend with a moving average filter, in order to remove high-frequency oscillations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "zdSZmN8bGT7a",
    "outputId": "0be137dd-c8bd-4acb-cf48-bca5c25e3f83"
   },
   "outputs": [],
   "source": [
    "px.line(load_series.rolling(365).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOeTz1GQGT7a"
   },
   "source": [
    "You then have a look at the **autocorrelation** function, a common statistical analysis which is very useful to detect and quantify seasonality patterns.\n",
    "\n",
    "The autocorrelation of a real-valued stochastic process $X(t)$ is a function of two time variables, defined as \n",
    "$$\n",
    "R_{XX}(t_1, t_2) = \\mathbb{E}\\left[X_{t_1}X_{t_2} \\right],\n",
    "$$\n",
    "and it measures the Pearson correlation between values of the $X$ process taken at those two times.\n",
    "\n",
    "In practice, usually we estimate the autocorrelation of a single realization of a process, i.e. a time series of the form $\\left\\{x(t)\\right\\}_{t=1}^n$, as a function of the difference $k = t_1 - t_2$, often called **lag**:\n",
    "$$\n",
    "\\hat{R}(k)=\\frac{1}{(n-k) \\sigma^2} \\sum_{t=1}^{n-k}\\left(x(t)-\\mu\\right)\\left(x(t+k)-\\mu\\right)\n",
    "$$\n",
    "where $\\mu$ and $\\sigma$ are also estimated from the time series.\n",
    "\n",
    "A high autocorrelation value for a given lag $k$ means that the series is highly correlated with a version of itself that has been shifted $k$ steps, which can be intuitively interpreted as the presence of a strong repetitive component having frequency $1/k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "gq9UG6rILu9i",
    "outputId": "58d6852f-5b5e-403d-c261-4b10a3401e87"
   },
   "outputs": [],
   "source": [
    "plot_acf(load_series, lags=370);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zAwsmdLUL0RM"
   },
   "source": [
    "Let's focus on the first lags to better appreciate the highest autocorrelation values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "yAN0JDWeGT7a",
    "outputId": "317be446-895a-4abe-df14-2835ec36befb"
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "plot_acf(load_series, lags=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zAH1DZipGT7b"
   },
   "source": [
    "The ACF shows a weekly seasonality, as well as a longer periodicity, which we may assume to be yearly. This fact can be confirmed by plotting the periodogram, an estimator of the spectral power density of the time series. The analysis is here omitted.\n",
    "\n",
    "Finally, you plot the power demand juxtaposed year-over-year, a view that is interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "uXsKHHXyGT7b",
    "outputId": "9d19bcbe-8734-4d52-cdd1-4df10c3d55da"
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "year_over_year_df = pd.DataFrame({\n",
    "    'load': load_series,\n",
    "    'day_in_year': load_series.index.dayofyear,\n",
    "    'day_in_week': load_series.index.dayofweek, # Monday = 0, Sunday = 6\n",
    "    'year': load_series.index.year\n",
    "})\n",
    "fig = go.Figure(\n",
    "    layout=dict(\n",
    "        title='Adjusted day in year - if each year started on Monday'\n",
    "    )\n",
    ")\n",
    "palette = iter(px.colors.sequential.Viridis[2:])\n",
    "for year, year_df in year_over_year_df.groupby('year'):\n",
    "  if year >= 2015:\n",
    "    year_df.day_in_year += year_df.day_in_week.iloc[0]\n",
    "    fig.add_trace(go.Scatter(x=year_df.day_in_year, y=year_df.load, name=year, marker_color=next(palette)))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ZvIFScyS7vT"
   },
   "source": [
    "# 4 - Modeling\n",
    "\n",
    "The modeling phase involves choosing a parameterized family of functions which we hope approximate well the target series and then run some algorithm that finds the best candidate within the family.\n",
    "\n",
    "Usually, one tries different models and then compares them to pick the best one; here we will just showcase a single good candidate model.\n",
    "\n",
    "In practice, after choosing the functional form of the model we will apply it on our data, which involves:\n",
    "- splitting the dataset at a certain date into two chunks;\n",
    "- using the first chunk to train the model, i.e. determining the parameters that provide the best fit to it;\n",
    "- using the second chunk to test how good the fit is when applied to unseen data.\n",
    "\n",
    "This subdivision is fundamental if we want to check that our model can generalize, i.e. perform well on new data that comes from the same generating process but exhibits different behaviors due to the stochastic nature of the process itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qco2y0hoobLc"
   },
   "source": [
    "## The model: Fourier regression\n",
    "\n",
    "Many methods for long-term forecasting aim at identifying and reconstructing the trend and seasonal structure of the series.\n",
    "When sesonal components are important, as in this case, a common approach is [Fourier regression](https://otexts.com/fpp3/useful-predictors.html#fourier-series).\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Simplification.</b> \n",
    "\n",
    "There are many academic papers and book chapters about the application of Fourier regression for time series forecasting: the interested reader may refer to [A. Incremona, Machine Learning methods for long and short term energy demand forecasting](https://iris.unipv.it/retrieve/handle/11571/1436355/470615/Incremona_PhD_Thesis.pdf), [A. Incremona, G. De Nicolao, Spectral Characterization of the Multi-Seasonal Component of the Italian Electric Load: A LASSO-FFT Approach](https://ieeexplore.ieee.org/abstract/document/8734852), and [A. Guerini, G. De Nicolao, Long-term electric load forecasting: A torus-based approach](https://ieeexplore.ieee.org/abstract/document/7330957).\n",
    "</div>\n",
    "\n",
    "Here, you opt for a simple unregularized linear model:\n",
    "$$\n",
    "y(t) = \\mu + \\kappa t + \\sum_{i=1}^n\\left[\\alpha_i\\sin(2\\pi\\frac{i}{7}t) + \\beta_i\\cos(2\\pi\\frac{i}{7}t)\\right] + \\sum_{j=1}^m\\left[\\gamma_j\\sin(2\\pi\\frac{j}{365.25}t) + \\delta_j\\cos(2\\pi\\frac{j}{365.25}t)\\right] + \\tau \\chi_t\n",
    "$$\n",
    "where:\n",
    "- the terms $\\mu + \\kappa t$ model a linear trend\n",
    "- $\\chi_t$ is the indicator function taking value 1 when date $t$ is a holiday and 0 otherwise\n",
    "- the sums\n",
    "$$\n",
    "\\sum_{i=1}^n\\left[\\alpha_i\\sin(2\\pi\\frac{i}{7}t) + \\beta_i\\cos(2\\pi\\frac{i}{7}t)\\right]\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\sum_{j=1}^m\\left[\\gamma_i\\sin(2\\pi\\frac{j}{365.25}t) + \\delta_j\\sin(2\\pi\\frac{j}{365.25}t)\\right]\n",
    "$$\n",
    "model the weekly and yearly periodicity\n",
    "\n",
    "The parameters of the model are $\\mu$, $\\kappa$, $\\{\\alpha_i\\}$, $\\{\\beta_i\\}$, $\\{\\gamma_j\\}$, $\\{\\delta_j\\}, \\tau$, which can all be trained by least squares. \n",
    "\n",
    "The hyperparameters are $n$ and $m$, which can be chosen by cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BMqMEa0STBco"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import holidays\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_VaWb11ot0C"
   },
   "source": [
    "## Feature preparation\n",
    "To perform a Fourier regression, you need proper inputs. Therefore, you build a new dataset.\n",
    "\n",
    "You add the trend and the holiday dummy, then you build the Fourier harmonics.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Simplification.</b> \n",
    "\n",
    "In the real world, it would be good practice to choose the number of harmonics with data-driven methods: cross-validation is one, LASSO is another.\n",
    "Here we just set a fixed value, which we know performs good enough for our purposes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzSP2190otTH"
   },
   "outputs": [],
   "source": [
    "n_harmonics = {\n",
    "    '7': 5,\n",
    "    '365': 20\n",
    "}\n",
    "\n",
    "dates = load_series.index\n",
    "regression_df = pd.DataFrame(index=dates)\n",
    "\n",
    "# create holiday series, valued 0 or 1 depending on the day\n",
    "holiday_cal = holidays.Italy()\n",
    "regression_df['holiday'] = dates.to_series().apply(lambda s: s in holiday_cal).astype(int)\n",
    "\n",
    "# turn dates into integer incremental values to be used as the x axis\n",
    "date_origin = pd.to_datetime('2006-01-01')\n",
    "regression_df['trend'] = (dates - date_origin).days\n",
    "\n",
    "# create fourier terms as trigonometric functions of various frequencies\n",
    "for i in range(1, n_harmonics['7'] + 1):\n",
    "    regression_df[f'sin_7_{i}'] = np.sin(2 * np.pi * i / 7 * regression_df['trend'])\n",
    "    regression_df[f'cos_7_{i}'] = np.cos(2 * np.pi * i / 7 * regression_df['trend'])\n",
    "for i in range(1, n_harmonics['365'] + 1):\n",
    "    regression_df[f'sin_365_{i}'] = np.sin(2 * np.pi * i / 365.25 * regression_df['trend'])\n",
    "    regression_df[f'cos_365_{i}'] = np.cos(2 * np.pi * i / 365.25 * regression_df['trend'])\n",
    "\n",
    "# join all features together\n",
    "regression_data_df = resampled_df.join(regression_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6S94KqTro-EK"
   },
   "source": [
    "## Model training\n",
    "\n",
    "Training the model is often done by using well-known libraries, such as [scikit-learn](https://scikit-learn.org/stable/); this makes everything straightforward, although it arguably hides the inner structure and complexity that ML algorithms have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6JxoWGMQona9"
   },
   "outputs": [],
   "source": [
    "TRAIN_END = '2020-12-31 23:59'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "id": "-Hf_nY-b-P26",
    "outputId": "e337c031-ec1f-4f3e-d8ea-578afafb988e"
   },
   "outputs": [],
   "source": [
    "# split data into train and test\n",
    "train_df, test_df = regression_data_df[:TRAIN_END], regression_data_df[TRAIN_END:]\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KMGmlsBAqjik"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ny23x3MwoqKb"
   },
   "outputs": [],
   "source": [
    "# split training set into features and target\n",
    "train_x, train_y = train_df.drop(columns='Load'), train_df['Load']\n",
    "test_x, test_y = test_df.drop(columns='Load'), test_df['Load']\n",
    "\n",
    "# instantiate and train scikit-learn's LinearRegression model\n",
    "model = LinearRegression()\n",
    "fit = model.fit(train_x, train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DaZxDronPrdu"
   },
   "outputs": [],
   "source": [
    "# save the model into a serialized format, ready to be used when needed\n",
    "import pickle\n",
    "with open(\"trained_model.pkl\", 'wb') as f:\n",
    "    pickle.dump(fit, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNH0M8rM6Gq4"
   },
   "source": [
    "## Model inspection\n",
    "\n",
    "Let's take a look at the output of the model, that is the parameters that were selected by the training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wz0rJvnG6JLL"
   },
   "outputs": [],
   "source": [
    "# extract information on estimated coefficients into a dataframe\n",
    "model_output_df = pd.DataFrame(index=pd.Index([\"intercept\"] + list(fit.feature_names_in_), name=\"component\"), data={\"coefficient\": [fit.intercept_] + list(fit.coef_)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "1ne64fR4cnJL",
    "outputId": "960b613a-e390-4d06-a361-686146ef90d2"
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact, widgets\n",
    "\n",
    "fig = px.bar(\n",
    "    np.abs(model_output_df),\n",
    "    title = \"Absolute value of estimated coefficients\",\n",
    "    log_y=True\n",
    "       )\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 574,
     "referenced_widgets": [
      "30e960ad24ba4cb98bce2d6c58f31bdc",
      "b7e6e79305584d8bb25b07187a912610",
      "b6c0eb2712164a1f84f1e3fd3ac4cc62",
      "d77d434bd33d4689a59c8423a395b2c0",
      "e0894b5ca254423fb742251bd8e8450a",
      "f89f0c2321cb4bacba7d557293535af4",
      "06576298e7b348678fdf5eb70fb0838f"
     ]
    },
    "id": "YT6ue51xhEyb",
    "outputId": "410f79d5-b4fe-4fe9-ebfe-57c3ddae14e3"
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "selector_widget = widgets.Dropdown(\n",
    "        description=f'Frequency:   ',\n",
    "        value=7,\n",
    "        options=[7, 365]\n",
    "    )\n",
    "@interact(frequency=selector_widget, name=\"Select frequency\")\n",
    "def plot_component(frequency):\n",
    "  if frequency==7:  # align index so that it starts on monday\n",
    "    start = 7 - date_origin.weekday()\n",
    "  else:\n",
    "    start = 0\n",
    "  index = np.arange(start, start + frequency)\n",
    "  component = pd.Series(0, index=index)\n",
    "\n",
    "  # reconstruct the predicted sum of components having the selected frequency\n",
    "  for i in range(1, n_harmonics[str(frequency)] + 1):\n",
    "\n",
    "    # sin component\n",
    "    sin_coeff = model_output_df.at[f\"sin_{frequency}_{i}\", \"coefficient\"]\n",
    "    harmonic_sin = np.sin(2 * np.pi * i / frequency * index) * sin_coeff\n",
    "    component += harmonic_sin\n",
    "\n",
    "    # cos component\n",
    "    cos_coeff = model_output_df.at[f\"cos_{frequency}_{i}\", \"coefficient\"]\n",
    "    harmonic_cos = np.cos(2 * np.pi * i / frequency * index) * cos_coeff\n",
    "    component += harmonic_cos\n",
    "\n",
    "  # set date index for better chart readability\n",
    "  date_index = pd.date_range(\n",
    "      date_origin + pd.offsets.DateOffset(days=start),\n",
    "      date_origin + pd.offsets.DateOffset(days=start + frequency - 1)\n",
    "      )\n",
    "  component.index = date_index\n",
    "  fig = px.line(component,\n",
    "                title = f\"Sum of predicted components with frequency={frequency}\",\n",
    "                width=1100)\n",
    "  if frequency==7:\n",
    "    fig.update_layout(xaxis = dict(tickformat = '%A'))\n",
    "  elif frequency==365:\n",
    "    fig.update_layout(xaxis = dict(tickformat = '%e %B'))\n",
    "  fig.update_layout(showlegend=False)\n",
    "  fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPJJVUgzp57u"
   },
   "source": [
    "## Prediction and evaluation\n",
    "Finally, we use the trained model to perform some predictions and evaluate the results. \n",
    "In order to do so, we will use the Mean Absolute Percentage Error:\n",
    "$$\n",
    "\\mathrm{MAPE(A, F)}=\\frac{100 \\%}{n} \\sum_{t=1}^n\\left|\\frac{A_t-F_t}{A_t}\\right|\n",
    "$$\n",
    "where $A$ and $F$ are the actual and forecasted series respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "TEyHStxWquH0",
    "outputId": "9a468204-551b-4b6f-9f2b-c2a9d06725a0"
   },
   "outputs": [],
   "source": [
    "# apply model on test dates\n",
    "test_predictions = model.predict(test_x)\n",
    "test_results_df = pd.DataFrame({\"actual\": test_df.Load, \"predicted\": test_predictions})\n",
    "px.line(test_results_df, title=\"Predictions on the test set\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uAOl3CqKroD0",
    "outputId": "175c2f28-be0f-40e0-f515-78bb1a1b5901"
   },
   "outputs": [],
   "source": [
    "# compute MAPE on test predictions\n",
    "np.mean(np.abs(test_results_df[\"actual\"] - test_results_df[\"predicted\"]) / test_results_df[\"predicted\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgUO_qMTvKF1"
   },
   "source": [
    "## Goodness of fit\n",
    "In order to assess the validity of a model, it is useful to plot actual vs predicted values and see how much they resemble an identity, which is plotted as the bisector of the first quadrant in the following chart.\n",
    "\n",
    "To help assess this, a line is fit to the scatter data by minimizing the sum of squared distances (OLS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "id": "r8QWFvCMWnr_",
    "outputId": "a57503f3-3d54-4941-d6f9-625d38679105"
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(test_results_df, x=\"actual\", y=\"predicted\", trendline=\"ols\", title=\"Goodness of fit\", height=800, width=900)\n",
    "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "fig.data[-1].marker.color = \"black\"\n",
    "fig.data[-1].name = \"OLS fit\"\n",
    "fig.data[-1].showlegend = True\n",
    "fig.add_trace(go.Scatter(x= np.arange(500000, 1000000, 5000), y=np.arange(500000, 1000000, 5000), mode = 'lines', name=\"actual=predicted\"))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VoLcGXbLwVJj"
   },
   "source": [
    "Similar information can be delivered by plotting the prediction errors (or residuals) against time in order to check \"problematic\" dates or periods where the error lies too far from the ideal zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3fiQrVDdw-ig"
   },
   "outputs": [],
   "source": [
    "resid_series = test_results_df[\"actual\"] - test_results_df[\"predicted\"]\n",
    "resid_series.name = \"Residual\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "FVPAFJdrw6SZ",
    "outputId": "504ade36-53b0-448f-dce3-81f72041409f"
   },
   "outputs": [],
   "source": [
    "px.line(resid_series, title=\"Difference between actual and predicted values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "goYZJhDCwx8s"
   },
   "source": [
    "Lastly, the same autocorrelation plot we used at the beginning shows us that the residual carries much less information than the original series in terms of repeating patterns; this means we extracted much of the predictable patterns that were encoded in the data, although we could definitely do something better as low lags still show significant autocorrelation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "fFT_EaFfaFLP",
    "outputId": "7204e77a-4c9a-4037-bd19-0201001cf297"
   },
   "outputs": [],
   "source": [
    "plot_acf(\n",
    "    resid_series,\n",
    "    lags=50,\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOipc_LSvPDq"
   },
   "source": [
    "# 5 - Model monitoring\n",
    "After settling for a satisfactory model, you need to keep monitoring its performance while it's being used by the company.\n",
    "\n",
    "For this, you set up a rolling measurement (moving average) of the model's performance, which tells you if it is behaving as expected or you need to worry about it, and potentially try better approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7VIMlf253G61"
   },
   "outputs": [],
   "source": [
    "future_df = pd.read_csv(\"data/2022_data.csv\", index_col=0, parse_dates=True).resample('D').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B0IFZBLl3pyU"
   },
   "outputs": [],
   "source": [
    "future_dates = future_df.index\n",
    "future_regression_df = pd.DataFrame(index=future_dates)\n",
    "\n",
    "future_regression_df['holiday'] = future_dates.to_series().apply(lambda s: s in holiday_cal).astype(int)\n",
    "\n",
    "future_regression_df['trend'] = (future_dates - date_origin).days\n",
    "\n",
    "for i in range(1, n_harmonics['7'] + 1):\n",
    "    future_regression_df[f'sin_7_{i}'] = np.sin(2 * np.pi * i / 7 * future_regression_df['trend'])\n",
    "    future_regression_df[f'cos_7_{i}'] = np.cos(2 * np.pi * i / 7 * future_regression_df['trend'])\n",
    "for i in range(1, n_harmonics['365'] + 1):\n",
    "    future_regression_df[f'sin_365_{i}'] = np.sin(2 * np.pi * i / 365.25 * future_regression_df['trend'])\n",
    "    future_regression_df[f'cos_365_{i}'] = np.cos(2 * np.pi * i / 365.25 * future_regression_df['trend'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i_FUOuyU3Kr_"
   },
   "outputs": [],
   "source": [
    "future_predictions = fit.predict(future_regression_df)\n",
    "future_results_df = pd.DataFrame({\"actual\": future_df.Load, \"predicted\": future_predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "o7Io1qDyvRBr",
    "outputId": "d3479289-e454-4af0-99f9-500cf61e9d14"
   },
   "outputs": [],
   "source": [
    "window_size = 3\n",
    "extended_future_results_df = pd.concat([test_results_df[-window_size:], future_results_df], axis=0)\n",
    "rolling_mape_df = pd.DataFrame(\n",
    "    {'rolling_mape': map(lambda w: np.mean(np.abs(w[\"actual\"] - w[\"predicted\"]) / w[\"predicted\"]),\n",
    "                         extended_future_results_df.rolling(window_size))},\n",
    "    index=extended_future_results_df.index\n",
    ")\n",
    "fig = px.line(rolling_mape_df[window_size:], title=f\"{window_size} days rolling MAPE in 2022\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yHyNExuYgW2z",
    "outputId": "05841e28-7f7c-4b88-dde7-b2603e5c61ff"
   },
   "outputs": [],
   "source": [
    "extended_future_results_df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Km6fmILuGT7Z"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
